{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619421a5-94a5-4581-806a-40e0477a7bd8",
   "metadata": {},
   "source": [
    "## 1. Read Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c97be6-8ed0-4bb8-be63-1aa107a90cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 23:48:28 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+-------------------+--------------------+---------------+--------------------+---------------+-----------------+--------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+--------------+---------+---------------+------------------------+----------------+-------------+-----------------------+------------------+------------+\n",
      "|             gmap_id|             address|avg_rating|            category|         description|               hours|          latitude|         longitude|          store_name|num_of_reviews|price|    relative_results|              state|  MISC_Accessibility|MISC_Activities|      MISC_Amenities|MISC_Atmosphere|       MISC_Crowd| MISC_Dining_options|MISC_From_the_business|MISC_Getting_here|MISC_Health_and_safety|MISC_Highlights|MISC_Lodging_options|      MISC_Offerings|       MISC_Payments|       MISC_Planning|    MISC_Popular_for|MISC_Recycling|MISC_Service_options|  zip|      zip_city|zip_state|     zip_county|irs_estimated_population|permanent_closed|price_numeric|avg_predicted_sentiment|     sentiment_std|review_count|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+-------------------+--------------------+---------------+--------------------+---------------+-----------------+--------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+--------------+---------+---------------+------------------------+----------------+-------------+-----------------------+------------------+------------+\n",
      "|0x89e85a90fa80ea7...|Checkers, 550 Wil...|       3.5|[Fast food restau...|Chain eatery serv...|[[Monday, 11AM–1A...|        40.7996343|-72.86699759999999|            Checkers|           426|    $|[0x89e85a90fe4fe4...|Closed ⋅ Opens 11AM|[Wheelchair acces...|           null|     [Good for kids]|       [Casual]|[Family-friendly]|[Breakfast, Lunch...|                  null|             null|                  null| [Fast service]|                null|[Kids' menu, Late...|[Debit cards, Cre...|                null|[Lunch, Dinner, S...|          null|[Outdoor seating,...|11967|       Shirley|       NY| Suffolk County|                   25560|               0|            1|     3.1702048057536985|1.0784551420278028|          11|\n",
      "|0x89c2c219189e518...|Momo Asian Fusion...|       4.6|[Japanese restaur...|                null|[[Friday, 11AM–9:...|41.121023199999996|       -73.9436054|   Momo Asian Fusion|            88| null|[0x89c2ea07be85c3...|Closed ⋅ Opens 11AM|[Wheelchair acces...|           null|[Good for kids, H...| [Casual, Cozy]|         [Groups]|           [Dessert]|                  null|             null|                  null|           null|                null|[Comfort food, He...|      [Credit cards]|[Accepts reservat...|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|10989|Valley Cottage|       NY|Rockland County|                    8770|               0|            0|       4.58498825475099|0.5092351974776369|           5|\n",
      "|0x8085b6a8ebae49f...|Rosso Pizzeria & ...|       4.2|[Pizza restaurant...|Neapolitan pizza,...|[[Saturday, 11:30...|           38.2328|      -122.6372028|Rosso Pizzeria & ...|            78| null|[0x8085b41d6183b8...| Permanently closed|                null|           null|     [Good for kids]|           null|             null|                null|                  null|             null|                  null|           null|                null|                null|                null|                null|                null|          null|          [Delivery]|94952|      Petaluma|       CA|  Sonoma County|                   28770|               1|            0|      3.864661623144518|0.9601397247395762|           5|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+-------------------+--------------------+---------------+--------------------+---------------+-----------------+--------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+--------------+---------+---------------+------------------------+----------------+-------------+-----------------------+------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "business_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/store_df/\")\n",
    "business_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b083390f-4bbe-47a2-8c54-a56ac9bb855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(business_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e2333-bc94-4669-91a4-549a534b5bcd",
   "metadata": {},
   "source": [
    "#### Apply Bayesian Shrinkage to Store Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6301569-c5fa-44fd-b860-c21a956cbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----------------------+------------+------------------+----------+\n",
      "|store_name                     |avg_predicted_sentiment|review_count|shrunk_sentiment  |avg_rating|\n",
      "+-------------------------------+-----------------------+------------+------------------+----------+\n",
      "|Checkers                       |3.1702048057536985     |11          |3.629562377589972 |3.5       |\n",
      "|Momo Asian Fusion              |4.58498825475099       |5           |4.284899889323579 |4.6       |\n",
      "|Rosso Pizzeria & Mozzarella Bar|3.864661623144518      |5           |4.044791012121421 |4.2       |\n",
      "|Junior's Restaurant & Bakery   |4.290628785509187      |15          |4.228319553949461 |4.4       |\n",
      "|Woori Village                  |3.1132834520223933     |10          |3.624069579316133 |3.5       |\n",
      "|The Flame Broiler              |4.007555898949703      |5           |4.092422437389816 |3.8       |\n",
      "|200 Fifth                      |4.116263523339339      |14          |4.124010266368728 |4.1       |\n",
      "|Wolffy's Grill and Marina      |4.237235247522416      |15          |4.196283431157399 |4.3       |\n",
      "|San Salvador Restaurant        |4.451223230894088      |15          |4.324676221180401 |4.3       |\n",
      "|La Herradura Inc II            |3.775857346585128      |5           |4.0151895866016245|4.1       |\n",
      "+-------------------------------+-----------------------+------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =========================\n",
    "# Parameters for shrinkage\n",
    "# =========================\n",
    "k = 10  # pseudo-count (adjust this to control shrinkage strength)\n",
    "\n",
    "# global mean of predicted sentiment across all stores\n",
    "global_mean = business_df.agg(F.mean(\"avg_predicted_sentiment\")).collect()[0][0]\n",
    "\n",
    "# =========================\n",
    "# Compute shrunk sentiment\n",
    "# =========================\n",
    "business_df = business_df.withColumn(\n",
    "    \"shrunk_sentiment\",\n",
    "    (F.col(\"avg_predicted_sentiment\") * F.col(\"review_count\") + global_mean * k) /\n",
    "    (F.col(\"review_count\") + k)\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Show results\n",
    "# =========================\n",
    "business_df.select(\n",
    "    \"store_name\",\n",
    "    \"avg_predicted_sentiment\",\n",
    "    \"review_count\",\n",
    "    \"shrunk_sentiment\",\n",
    "    \"avg_rating\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755afea-e36d-407b-8b24-d5446e1b6167",
   "metadata": {},
   "source": [
    "#### Checking Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7b9948-a944-4809-8d7c-d293a86e5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------+\n",
      "|column                  |missing_count|\n",
      "+------------------------+-------------+\n",
      "|gmap_id                 |0            |\n",
      "|address                 |0            |\n",
      "|avg_rating              |0            |\n",
      "|category                |0            |\n",
      "|description             |72284        |\n",
      "|hours                   |8333         |\n",
      "|latitude                |0            |\n",
      "|longitude               |0            |\n",
      "|store_name              |0            |\n",
      "|num_of_reviews          |0            |\n",
      "|price                   |47163        |\n",
      "|relative_results        |12725        |\n",
      "|state                   |46186        |\n",
      "|MISC_Accessibility      |33726        |\n",
      "|MISC_Activities         |169478       |\n",
      "|MISC_Amenities          |26983        |\n",
      "|MISC_Atmosphere         |38927        |\n",
      "|MISC_Crowd              |52454        |\n",
      "|MISC_Dining_options     |50973        |\n",
      "|MISC_From_the_business  |163447       |\n",
      "|MISC_Getting_here       |169478       |\n",
      "|MISC_Health_and_safety  |168116       |\n",
      "|MISC_Highlights         |91905        |\n",
      "|MISC_Lodging_options    |169478       |\n",
      "|MISC_Offerings          |25206        |\n",
      "|MISC_Payments           |74194        |\n",
      "|MISC_Planning           |117245       |\n",
      "|MISC_Popular_for        |44101        |\n",
      "|MISC_Recycling          |169477       |\n",
      "|MISC_Service_options    |3470         |\n",
      "|zip                     |0            |\n",
      "|zip_city                |0            |\n",
      "|zip_state               |0            |\n",
      "|zip_county              |0            |\n",
      "|irs_estimated_population|0            |\n",
      "|permanent_closed        |0            |\n",
      "|price_numeric           |0            |\n",
      "|avg_predicted_sentiment |0            |\n",
      "|sentiment_std           |0            |\n",
      "|review_count            |0            |\n",
      "|shrunk_sentiment        |0            |\n",
      "+------------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# compute missing counts (one row)\n",
    "missing_df = business_df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in business_df.columns\n",
    "])\n",
    "\n",
    "# collect as Python dict\n",
    "missing_dict = missing_df.first().asDict()\n",
    "\n",
    "# convert to list of Rows\n",
    "rows = [Row(column=col_name, missing_count=missing_dict[col_name])\n",
    "        for col_name in missing_dict]\n",
    "\n",
    "# create a transposed Spark DataFrame\n",
    "transposed_missing_df = spark.createDataFrame(rows)\n",
    "\n",
    "# show results\n",
    "transposed_missing_df.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0537d322-2bf8-4c2a-9f10-eb6ee0e39110",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"MISC_Activities\", \"MISC_From_the_business\", \"MISC_Getting_here\",\n",
    "    \"MISC_Health_and_safety\", \"MISC_Lodging_options\", \"MISC_Recycling\"\n",
    "    \"review_count\", \n",
    "]\n",
    "\n",
    "business_df_clean = business_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f9bbf0-75f7-4cf4-be1d-d8d4f22a7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "business_df_clean = business_df_clean.withColumn(\"irs_estimated_population\", col(\"irs_estimated_population\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90ec51-77fd-4469-999b-5433467e1568",
   "metadata": {},
   "source": [
    "#### One-Hot Encode Top 3 Categories for Each Categorical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefe26db-6a0d-4c9f-93db-9722820f52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, when, array_contains\n",
    "\n",
    "misc_cols = [\n",
    "    \"MISC_Accessibility\", \"MISC_Amenities\", \"MISC_Atmosphere\",\n",
    "    \"MISC_Crowd\", \"MISC_Dining_options\", \"MISC_Offerings\",\n",
    "    \"MISC_Payments\", \"MISC_Planning\", \"MISC_Popular_for\",\n",
    "    \"MISC_Service_options\", \"MISC_Highlights\"\n",
    "]\n",
    "\n",
    "for c in misc_cols:\n",
    "    # Explode the array to count frequency\n",
    "    top_items = (business_df_clean\n",
    "                 .select(explode(col(c)).alias(\"item\"))\n",
    "                 .groupBy(\"item\")\n",
    "                 .count()\n",
    "                 .orderBy(col(\"count\").desc())\n",
    "                 .limit(3)\n",
    "                 .collect())\n",
    "    \n",
    "    top_items_list = [row[\"item\"] for row in top_items if row[\"item\"] is not None]\n",
    "\n",
    "    # Create one-hot columns for top 3\n",
    "    for item in top_items_list:\n",
    "        col_name = f\"{c}_{item.replace(' ', '_')}_flag\"\n",
    "        business_df_clean = business_df_clean.withColumn(\n",
    "            col_name,\n",
    "            when(array_contains(col(c), item), 1).otherwise(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3a072e-26c9-4185-8cfb-7425c6611e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+-------------------+--------------------+--------------------+---------------+-----------------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+--------------+---------+---------------+------------------------+----------------+-------------+-----------------------+------------------+------------+-----------------+------------------------------------------------------+------------------------------------------------------+---------------------------------------------------------+---------------------------------+-------------------------------+----------------------------+---------------------------+-------------------------+-----------------------------+----------------------+------------------------+-------------------------------+--------------------------------+------------------------------+-------------------------------+--------------------------------+------------------------------+--------------------------+--------------------------------------+------------------------------+-------------------------------+---------------------------------------+---------------------------------+--------------------------------------------------+---------------------------------+---------------------------+----------------------------+----------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+----------------------------------------+\n",
      "|             gmap_id|             address|avg_rating|            category|         description|               hours|          latitude|         longitude|          store_name|num_of_reviews|price|    relative_results|              state|  MISC_Accessibility|      MISC_Amenities|MISC_Atmosphere|       MISC_Crowd| MISC_Dining_options|MISC_Highlights|      MISC_Offerings|       MISC_Payments|       MISC_Planning|    MISC_Popular_for|MISC_Recycling|MISC_Service_options|  zip|      zip_city|zip_state|     zip_county|irs_estimated_population|permanent_closed|price_numeric|avg_predicted_sentiment|     sentiment_std|review_count| shrunk_sentiment|MISC_Accessibility_Wheelchair_accessible_entrance_flag|MISC_Accessibility_Wheelchair_accessible_restroom_flag|MISC_Accessibility_Wheelchair_accessible_parking_lot_flag|MISC_Amenities_Good_for_kids_flag|MISC_Amenities_High_chairs_flag|MISC_Amenities_Restroom_flag|MISC_Atmosphere_Casual_flag|MISC_Atmosphere_Cozy_flag|MISC_Atmosphere_Romantic_flag|MISC_Crowd_Groups_flag|MISC_Crowd_Tourists_flag|MISC_Crowd_Family-friendly_flag|MISC_Dining_options_Dessert_flag|MISC_Dining_options_Lunch_flag|MISC_Dining_options_Dinner_flag|MISC_Offerings_Comfort_food_flag|MISC_Offerings_Quick_bite_flag|MISC_Offerings_Coffee_flag|MISC_Payments_NFC_mobile_payments_flag|MISC_Payments_Debit_cards_flag|MISC_Payments_Credit_cards_flag|MISC_Planning_Accepts_reservations_flag|MISC_Planning_LGBTQ_friendly_flag|MISC_Planning_Dinner_reservations_recommended_flag|MISC_Popular_for_Solo_dining_flag|MISC_Popular_for_Lunch_flag|MISC_Popular_for_Dinner_flag|MISC_Service_options_Delivery_flag|MISC_Service_options_Takeout_flag|MISC_Service_options_Dine-in_flag|MISC_Highlights_Fast_service_flag|MISC_Highlights_Great_coffee_flag|MISC_Highlights_Great_tea_selection_flag|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+-------------------+--------------------+--------------------+---------------+-----------------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+--------------+---------+---------------+------------------------+----------------+-------------+-----------------------+------------------+------------+-----------------+------------------------------------------------------+------------------------------------------------------+---------------------------------------------------------+---------------------------------+-------------------------------+----------------------------+---------------------------+-------------------------+-----------------------------+----------------------+------------------------+-------------------------------+--------------------------------+------------------------------+-------------------------------+--------------------------------+------------------------------+--------------------------+--------------------------------------+------------------------------+-------------------------------+---------------------------------------+---------------------------------+--------------------------------------------------+---------------------------------+---------------------------+----------------------------+----------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+----------------------------------------+\n",
      "|0x89e85a90fa80ea7...|Checkers, 550 Wil...|       3.5|[Fast food restau...|Chain eatery serv...|[[Monday, 11AM–1A...|        40.7996343|-72.86699759999999|            Checkers|           426|    $|[0x89e85a90fe4fe4...|Closed ⋅ Opens 11AM|[Wheelchair acces...|     [Good for kids]|       [Casual]|[Family-friendly]|[Breakfast, Lunch...| [Fast service]|[Kids' menu, Late...|[Debit cards, Cre...|                null|[Lunch, Dinner, S...|          null|[Outdoor seating,...|11967|       Shirley|       NY| Suffolk County|                 25560.0|               0|            1|     3.1702048057536985|1.0784551420278028|          11|3.629562377589972|                                                     1|                                                     1|                                                        1|                                1|                              0|                           0|                          1|                        0|                            0|                     0|                       0|                              1|                               1|                             1|                              1|                               0|                             1|                         0|                                     0|                             1|                              1|                                      0|                                0|                                                 0|                                1|                          1|                           1|                                 1|                                1|                                1|                                1|                                0|                                       0|\n",
      "|0x89c2c219189e518...|Momo Asian Fusion...|       4.6|[Japanese restaur...|                null|[[Friday, 11AM–9:...|41.121023199999996|       -73.9436054|   Momo Asian Fusion|            88| null|[0x89c2ea07be85c3...|Closed ⋅ Opens 11AM|[Wheelchair acces...|[Good for kids, H...| [Casual, Cozy]|         [Groups]|           [Dessert]|           null|[Comfort food, He...|      [Credit cards]|[Accepts reservat...|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|10989|Valley Cottage|       NY|Rockland County|                  8770.0|               0|            0|       4.58498825475099|0.5092351974776369|           5|4.284899889323579|                                                     1|                                                     0|                                                        0|                                1|                              1|                           0|                          1|                        1|                            0|                     1|                       0|                              0|                               1|                             0|                              0|                               1|                             1|                         0|                                     0|                             0|                              1|                                      1|                                0|                                                 0|                                1|                          1|                           1|                                 1|                                1|                                1|                                0|                                0|                                       0|\n",
      "|0x8085b6a8ebae49f...|Rosso Pizzeria & ...|       4.2|[Pizza restaurant...|Neapolitan pizza,...|[[Saturday, 11:30...|           38.2328|      -122.6372028|Rosso Pizzeria & ...|            78| null|[0x8085b41d6183b8...| Permanently closed|                null|     [Good for kids]|           null|             null|                null|           null|                null|                null|                null|                null|          null|          [Delivery]|94952|      Petaluma|       CA|  Sonoma County|                 28770.0|               1|            0|      3.864661623144518|0.9601397247395762|           5|4.044791012121421|                                                     0|                                                     0|                                                        0|                                1|                              0|                           0|                          0|                        0|                            0|                     0|                       0|                              0|                               0|                             0|                              0|                               0|                             0|                         0|                                     0|                             0|                              0|                                      0|                                0|                                                 0|                                0|                          0|                           0|                                 1|                                0|                                0|                                0|                                0|                                       0|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+-------------------+--------------------+--------------------+---------------+-----------------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+--------------+---------+---------------+------------------------+----------------+-------------+-----------------------+------------------+------------+-----------------+------------------------------------------------------+------------------------------------------------------+---------------------------------------------------------+---------------------------------+-------------------------------+----------------------------+---------------------------+-------------------------+-----------------------------+----------------------+------------------------+-------------------------------+--------------------------------+------------------------------+-------------------------------+--------------------------------+------------------------------+--------------------------+--------------------------------------+------------------------------+-------------------------------+---------------------------------------+---------------------------------+--------------------------------------------------+---------------------------------+---------------------------+----------------------------+----------------------------------+---------------------------------+---------------------------------+---------------------------------+---------------------------------+----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business_df_clean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a996a-2798-43c1-bf6a-03bfa3f78cf3",
   "metadata": {},
   "source": [
    "## 2. Compare Model Performance for Predicting Closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3c323c-f7b3-4865-82a8-234521ce1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: 1.0 6.365726454865487\n",
      "Class counts: 146469 23009\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, isnan\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Feature Columns\n",
    "# -----------------------------------------------------\n",
    "feature_cols = [\n",
    "    \"avg_rating\", \n",
    "    \"price_numeric\", \n",
    "    \"num_of_reviews\", \n",
    "    \"irs_estimated_population\",\n",
    "    \"shrunk_sentiment\", \n",
    "    \"sentiment_std\",\n",
    "]\n",
    "\n",
    "one_hot_cols = [c for c in business_df_clean.columns if c.endswith(\"_flag\")] \n",
    "feature_cols += one_hot_cols\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Check for missing columns (safety)\n",
    "# -----------------------------------------------------\n",
    "missing = [c for c in feature_cols if c not in business_df_clean.columns]\n",
    "if missing:\n",
    "    raise Exception(f\"Missing required feature columns: {missing}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Clean Null values\n",
    "# -----------------------------------------------------\n",
    "for c in feature_cols:\n",
    "    business_df_clean = business_df_clean.withColumn(\n",
    "        c, when(isnan(col(c)) | col(c).isNull(), 0).otherwise(col(c))\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Compute class weights\n",
    "# -----------------------------------------------------\n",
    "counts = business_df_clean.groupBy(\"permanent_closed\").count().collect()\n",
    "count_0 = next(r['count'] for r in counts if r['permanent_closed'] == 0)\n",
    "count_1 = next(r['count'] for r in counts if r['permanent_closed'] == 1)\n",
    "\n",
    "majority = max(count_0, count_1)\n",
    "minority = min(count_0, count_1)\n",
    "\n",
    "weight_for_0 = majority / count_0\n",
    "weight_for_1 = majority / count_1\n",
    "\n",
    "business_df_clean = business_df_clean.withColumn(\n",
    "    \"classWeight\",\n",
    "    when(col(\"permanent_closed\") == 0, weight_for_0).otherwise(weight_for_1)\n",
    ")\n",
    "\n",
    "print(\"Class Weights:\", weight_for_0, weight_for_1)\n",
    "print(\"Class counts:\", count_0, count_1)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Train/test split\n",
    "# -----------------------------------------------------\n",
    "train_df, test_df = business_df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Assemble features\n",
    "# -----------------------------------------------------\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"   # <---- prevents schema errors\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Evaluators\n",
    "# -----------------------------------------------------\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"permanent_closed\", metricName=\"areaUnderROC\")\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"permanent_closed\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"permanent_closed\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"permanent_closed\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"permanent_closed\", metricName=\"weightedRecall\")\n",
    "\n",
    "def evaluate_model(predictions, name=\"Model\"):\n",
    "    auc = evaluator_auc.evaluate(predictions)\n",
    "    acc = evaluator_acc.evaluate(predictions)\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "    print(f\"\\n===== {name} Evaluation =====\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d1a87-10c7-4ac4-9b4e-78c884fb8f35",
   "metadata": {},
   "source": [
    "#### Logistic Regression (Class Weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35b76bb-d075-4b29-8320-4dab8d902fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 117438, Class 1: 18419, Weight ratio: 6.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 23:49:33 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "25/12/03 23:49:33 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Logistic Regression (Weighted) Evaluation =====\n",
      "AUC:       0.9588\n",
      "Accuracy:  0.8941\n",
      "F1 Score:  0.9044\n",
      "Precision: 0.9325\n",
      "Recall:    0.8941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2142:==============================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Confusion Matrix =====\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0      25793.0       3308.0\n",
      "Actual 1        258.0       4307.0\n",
      "\n",
      "===== Logistic Regression Feature Importance =====\n",
      "                                                  feature   coefficient\n",
      "                              MISC_Atmosphere_Casual_flag -1.494303e+00\n",
      "                        MISC_Service_options_Takeout_flag -1.411024e+00\n",
      "                        MISC_Amenities_Good_for_kids_flag  1.248397e+00\n",
      "                        MISC_Popular_for_Solo_dining_flag -1.241674e+00\n",
      "                           MISC_Offerings_Quick_bite_flag -1.228908e+00\n",
      "                        MISC_Service_options_Dine-in_flag -1.206833e+00\n",
      "                           MISC_Payments_Debit_cards_flag  1.159994e+00\n",
      "       MISC_Planning_Dinner_reservations_recommended_flag -1.032567e+00\n",
      "                       MISC_Service_options_Delivery_flag  9.973183e-01\n",
      "                   MISC_Payments_NFC_mobile_payments_flag -9.904928e-01\n",
      "                          MISC_Payments_Credit_cards_flag -9.786995e-01\n",
      "   MISC_Accessibility_Wheelchair_accessible_entrance_flag -8.797833e-01\n",
      "                                 MISC_Crowd_Tourists_flag -7.523490e-01\n",
      "                                         shrunk_sentiment  6.928483e-01\n",
      "                          MISC_Dining_options_Dinner_flag  5.871716e-01\n",
      "                          MISC_Crowd_Family-friendly_flag -5.140859e-01\n",
      "                  MISC_Planning_Accepts_reservations_flag  4.151350e-01\n",
      "                               MISC_Offerings_Coffee_flag  3.990187e-01\n",
      "   MISC_Accessibility_Wheelchair_accessible_restroom_flag -3.887883e-01\n",
      "                             MISC_Popular_for_Dinner_flag -3.135376e-01\n",
      "                           MISC_Dining_options_Lunch_flag  3.047988e-01\n",
      "                                MISC_Atmosphere_Cozy_flag -2.577797e-01\n",
      "                                               avg_rating -2.165704e-01\n",
      "                         MISC_Offerings_Comfort_food_flag  1.939138e-01\n",
      "                             MISC_Amenities_Restroom_flag  1.924292e-01\n",
      "                            MISC_Atmosphere_Romantic_flag  1.625623e-01\n",
      "                 MISC_Highlights_Great_tea_selection_flag  1.578890e-01\n",
      "                                            price_numeric  1.322751e-01\n",
      "MISC_Accessibility_Wheelchair_accessible_parking_lot_flag -9.083939e-02\n",
      "                          MISC_Amenities_High_chairs_flag  8.212469e-02\n",
      "                        MISC_Highlights_Great_coffee_flag -7.359092e-02\n",
      "                              MISC_Popular_for_Lunch_flag  6.360802e-02\n",
      "                        MISC_Planning_LGBTQ_friendly_flag  4.612636e-02\n",
      "                        MISC_Highlights_Fast_service_flag -3.373448e-02\n",
      "                                   MISC_Crowd_Groups_flag  2.985871e-02\n",
      "                                            sentiment_std -2.547192e-02\n",
      "                         MISC_Dining_options_Dessert_flag  1.215931e-02\n",
      "                                           num_of_reviews -2.740672e-03\n",
      "                                 irs_estimated_population  3.532572e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 1. Handle class imbalance \n",
    "# ============================================================\n",
    "\n",
    "# Count classes\n",
    "counts = train_df.groupBy(\"permanent_closed\").count().collect()\n",
    "count_0 = [r[\"count\"] for r in counts if r[\"permanent_closed\"] == 0][0]\n",
    "count_1 = [r[\"count\"] for r in counts if r[\"permanent_closed\"] == 1][0]\n",
    "\n",
    "# Assign weights: majority = 1, minority = ratio\n",
    "ratio = count_0 / count_1\n",
    "train_df_balanced = train_df.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"permanent_closed\") == 1, ratio).otherwise(1.0)\n",
    ")\n",
    "\n",
    "print(f\"Class 0: {count_0}, Class 1: {count_1}, Weight ratio: {ratio:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Logistic Regression with CV\n",
    "# ============================================================\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"permanent_closed\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=25\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "paramGrid_lr = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.0, 0.01, 0.1])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cv_lr = CrossValidator(\n",
    "    estimator=lr_pipeline,\n",
    "    estimatorParamMaps=paramGrid_lr,\n",
    "    evaluator=evaluator_auc,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "cv_model_lr = cv_lr.fit(train_df_balanced)\n",
    "predictions_lr = cv_model_lr.transform(test_df)\n",
    "\n",
    "evaluate_model(predictions_lr, \"Logistic Regression (Weighted)\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Confusion Matrix\n",
    "# ============================================================\n",
    "\n",
    "predictionAndLabels = predictions_lr.select(\"prediction\", \"permanent_closed\") \\\n",
    "                                   .rdd.map(lambda r: (float(r[\"prediction\"]), float(r[\"permanent_closed\"])))\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "conf_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "conf_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== Confusion Matrix =====\")\n",
    "print(conf_df)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Feature Importance (LR coefficients)\n",
    "# ============================================================\n",
    "\n",
    "lr_model = cv_model_lr.bestModel.stages[-1]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coefficient\": lr_model.coefficients\n",
    "})\n",
    "\n",
    "feature_importance[\"abs_coeff\"] = feature_importance[\"coefficient\"].abs()\n",
    "feature_importance = feature_importance.sort_values(\"abs_coeff\", ascending=False)\n",
    "\n",
    "print(\"\\n===== Logistic Regression Feature Importance =====\")\n",
    "print(feature_importance[[\"feature\", \"coefficient\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc5180-5309-4e44-a432-ee053de08fe9",
   "metadata": {},
   "source": [
    "#### Random Forest (Class Weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653284ff-f572-415f-ab15-aefaa0a23719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Set Spark log level to WARN or ERROR (suppress INFO and repeated WARN)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Suppress some specific loggers\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "log4jLogger.LogManager.getLogger(\"org.apache.spark.scheduler.DAGScheduler\").setLevel(log4jLogger.Level.ERROR)\n",
    "log4jLogger.LogManager.getLogger(\"org.apache.spark.storage.BlockManager\").setLevel(log4jLogger.Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e7b64e-b11c-4c4b-b5e8-f6a6f85f0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest (Weighted) Evaluation =====\n",
      "AUC:       0.9714\n",
      "Accuracy:  0.9080\n",
      "F1 Score:  0.9166\n",
      "Precision: 0.9417\n",
      "Recall:    0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2587:===>                                                  (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest Confusion Matrix =====\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0      26122.0       2979.0\n",
      "Actual 1        119.0       4446.0\n",
      "\n",
      "===== Random Forest Feature Importance =====\n",
      "                                                  feature  importance\n",
      "                        MISC_Popular_for_Solo_dining_flag    0.160665\n",
      "                              MISC_Atmosphere_Casual_flag    0.143360\n",
      "                           MISC_Offerings_Quick_bite_flag    0.138450\n",
      "                        MISC_Service_options_Takeout_flag    0.100326\n",
      "                              MISC_Popular_for_Lunch_flag    0.058737\n",
      "                                           num_of_reviews    0.058456\n",
      "                             MISC_Popular_for_Dinner_flag    0.057672\n",
      "                        MISC_Service_options_Dine-in_flag    0.054443\n",
      "                                   MISC_Crowd_Groups_flag    0.034572\n",
      "   MISC_Accessibility_Wheelchair_accessible_entrance_flag    0.031464\n",
      "                                 MISC_Crowd_Tourists_flag    0.020366\n",
      "                        MISC_Amenities_Good_for_kids_flag    0.020327\n",
      "                   MISC_Payments_NFC_mobile_payments_flag    0.012525\n",
      "                         MISC_Dining_options_Dessert_flag    0.010663\n",
      "                         MISC_Offerings_Comfort_food_flag    0.009734\n",
      "                                MISC_Atmosphere_Cozy_flag    0.009472\n",
      "                          MISC_Dining_options_Dinner_flag    0.007945\n",
      "                           MISC_Dining_options_Lunch_flag    0.007700\n",
      "                           MISC_Payments_Debit_cards_flag    0.007305\n",
      "                       MISC_Service_options_Delivery_flag    0.006210\n",
      "                          MISC_Amenities_High_chairs_flag    0.005914\n",
      "                          MISC_Payments_Credit_cards_flag    0.005484\n",
      "                                            price_numeric    0.005277\n",
      "                                               avg_rating    0.004602\n",
      "                        MISC_Highlights_Fast_service_flag    0.004178\n",
      "                          MISC_Crowd_Family-friendly_flag    0.002646\n",
      "                                         shrunk_sentiment    0.002580\n",
      "                        MISC_Highlights_Great_coffee_flag    0.002473\n",
      "                  MISC_Planning_Accepts_reservations_flag    0.002415\n",
      "                             MISC_Amenities_Restroom_flag    0.002321\n",
      "                                 irs_estimated_population    0.002248\n",
      "                                            sentiment_std    0.002006\n",
      "       MISC_Planning_Dinner_reservations_recommended_flag    0.001582\n",
      "                               MISC_Offerings_Coffee_flag    0.001528\n",
      "MISC_Accessibility_Wheelchair_accessible_parking_lot_flag    0.001506\n",
      "   MISC_Accessibility_Wheelchair_accessible_restroom_flag    0.001254\n",
      "                            MISC_Atmosphere_Romantic_flag    0.000591\n",
      "                 MISC_Highlights_Great_tea_selection_flag    0.000574\n",
      "                        MISC_Planning_LGBTQ_friendly_flag    0.000430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Handle class imbalance\n",
    "# ---------------------------------------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"permanent_closed\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\"\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "                .addGrid(rf.maxDepth, [5, 10])\n",
    "                .addGrid(rf.numTrees, [50, 100])\n",
    "                .build())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Random Forest CV\n",
    "# ---------------------------------------------------------\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=rf_pipeline,\n",
    "    estimatorParamMaps=paramGrid_rf,\n",
    "    evaluator=evaluator_auc,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "cv_model_rf = cv_rf.fit(train_df_balanced)\n",
    "predictions_rf = cv_model_rf.transform(test_df)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Confusion Matrix\n",
    "# ---------------------------------------------------------\n",
    "evaluate_model(predictions_rf, \"Random Forest (Weighted)\")\n",
    "\n",
    "predictionAndLabels = predictions_rf.select(\"prediction\", \"permanent_closed\") \\\n",
    "                                   .rdd.map(lambda row: (float(row['prediction']), float(row['permanent_closed'])))\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "conf_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "print(\"\\n===== Random Forest Confusion Matrix =====\")\n",
    "conf_df = pd.DataFrame(conf_matrix, \n",
    "                       index=[\"Actual 0\", \"Actual 1\"], \n",
    "                       columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "print(conf_df)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Feature Importance\n",
    "# ---------------------------------------------------------\n",
    "rf_model = cv_model_rf.bestModel.stages[-1]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": rf_model.featureImportances.toArray()\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n===== Random Forest Feature Importance =====\")\n",
    "print(feature_importance.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58ee1b-8a7b-46a5-ae4b-c422808fdf34",
   "metadata": {},
   "source": [
    "#### Gradient-Boosted Trees (Class Weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a4ae3c9-6518-429a-8f28-70e725e03113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Gradient-Boosted Trees Evaluation =====\n",
      "AUC:       0.9717\n",
      "Accuracy:  0.9120\n",
      "F1 Score:  0.9198\n",
      "Precision: 0.9422\n",
      "Recall:    0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6656:===========================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Gradient-Boosted Trees Confusion Matrix =====\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0      26293.0       2808.0\n",
      "Actual 1        155.0       4410.0\n",
      "\n",
      "===== Gradient-Boosted Trees Feature Importance =====\n",
      "                                                  feature  importance\n",
      "                        MISC_Popular_for_Solo_dining_flag    0.443764\n",
      "                        MISC_Service_options_Takeout_flag    0.119859\n",
      "                                           num_of_reviews    0.086911\n",
      "                              MISC_Atmosphere_Casual_flag    0.053423\n",
      "                        MISC_Amenities_Good_for_kids_flag    0.035284\n",
      "                        MISC_Service_options_Dine-in_flag    0.030336\n",
      "                         MISC_Offerings_Comfort_food_flag    0.030047\n",
      "   MISC_Accessibility_Wheelchair_accessible_entrance_flag    0.030043\n",
      "                                            price_numeric    0.019744\n",
      "                  MISC_Planning_Accepts_reservations_flag    0.016096\n",
      "                           MISC_Payments_Debit_cards_flag    0.014671\n",
      "                           MISC_Offerings_Quick_bite_flag    0.012362\n",
      "                   MISC_Payments_NFC_mobile_payments_flag    0.011423\n",
      "                                               avg_rating    0.011250\n",
      "                           MISC_Dining_options_Lunch_flag    0.007994\n",
      "                          MISC_Dining_options_Dinner_flag    0.007883\n",
      "                       MISC_Service_options_Delivery_flag    0.007092\n",
      "                             MISC_Amenities_Restroom_flag    0.006583\n",
      "                                   MISC_Crowd_Groups_flag    0.006291\n",
      "                                 irs_estimated_population    0.005713\n",
      "                          MISC_Payments_Credit_cards_flag    0.004909\n",
      "   MISC_Accessibility_Wheelchair_accessible_restroom_flag    0.004556\n",
      "                                MISC_Atmosphere_Cozy_flag    0.003779\n",
      "                             MISC_Popular_for_Dinner_flag    0.003743\n",
      "                          MISC_Crowd_Family-friendly_flag    0.003356\n",
      "                                         shrunk_sentiment    0.003177\n",
      "                               MISC_Offerings_Coffee_flag    0.003172\n",
      "                         MISC_Dining_options_Dessert_flag    0.002949\n",
      "                              MISC_Popular_for_Lunch_flag    0.002940\n",
      "                          MISC_Amenities_High_chairs_flag    0.002229\n",
      "                            MISC_Atmosphere_Romantic_flag    0.002091\n",
      "MISC_Accessibility_Wheelchair_accessible_parking_lot_flag    0.001482\n",
      "       MISC_Planning_Dinner_reservations_recommended_flag    0.001411\n",
      "                                 MISC_Crowd_Tourists_flag    0.000917\n",
      "                                            sentiment_std    0.000703\n",
      "                 MISC_Highlights_Great_tea_selection_flag    0.000563\n",
      "                        MISC_Highlights_Fast_service_flag    0.000551\n",
      "                        MISC_Planning_LGBTQ_friendly_flag    0.000467\n",
      "                        MISC_Highlights_Great_coffee_flag    0.000237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Handle class imbalance\n",
    "# ---------------------------------------------------------\n",
    "# Compute class weights \n",
    "counts = train_df.groupBy(\"permanent_closed\").count().toPandas()\n",
    "n0 = counts[counts[\"permanent_closed\"] == 0][\"count\"].values[0]\n",
    "n1 = counts[counts[\"permanent_closed\"] == 1][\"count\"].values[0]\n",
    "\n",
    "minority_weight = max(n0, n1) / min(n0, n1)\n",
    "\n",
    "# Attach weight column\n",
    "train_df = train_df.withColumn(\n",
    "    \"weight\",\n",
    "    (1.0 * (train_df.permanent_closed == 1).cast(\"int\")) * minority_weight +\n",
    "    (1.0 * (train_df.permanent_closed == 0).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Gradient-Boosted Trees CV\n",
    "# ---------------------------------------------------------\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"permanent_closed\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=50,   # upper bound, CV will tune\n",
    "    maxDepth=5    # upper bound, CV will tune\n",
    ")\n",
    "\n",
    "gbt_pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "paramGrid_gbt = (ParamGridBuilder()\n",
    "                 .addGrid(gbt.maxDepth, [3, 5])\n",
    "                 .addGrid(gbt.maxIter, [20, 50])\n",
    "                 .build())\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=gbt_pipeline,\n",
    "    estimatorParamMaps=paramGrid_gbt,\n",
    "    evaluator=evaluator_auc,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "cv_model_gbt = cv_gbt.fit(train_df)\n",
    "predictions_gbt = cv_model_gbt.transform(test_df)\n",
    "\n",
    "evaluate_model(predictions_gbt, \"Gradient-Boosted Trees\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Confusion Matrix\n",
    "# ---------------------------------------------------------\n",
    "predictionAndLabels = predictions_gbt.select(\"prediction\", \"permanent_closed\") \\\n",
    "    .rdd.map(lambda row: (float(row[\"prediction\"]), float(row[\"permanent_closed\"])))\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "conf_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "print(\"\\n===== Gradient-Boosted Trees Confusion Matrix =====\")\n",
    "conf_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(conf_df)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Feature Importance\n",
    "# ---------------------------------------------------------\n",
    "gbt_model = cv_model_gbt.bestModel.stages[-1]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": gbt_model.featureImportances.toArray()\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n===== Gradient-Boosted Trees Feature Importance =====\")\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f2aa8-c8c1-4ff2-8fc5-a1c710da2465",
   "metadata": {},
   "source": [
    "## 3. Interpret Feature Importance and Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52629d39-b984-4007-b030-13624bdc3807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############################################################\n",
      "###  Top Predictors with GBT Importance and Direction ###\n",
      "#############################################################\n",
      "\n",
      "                                  Feature_Clean  GBT_Importance  ENet_Coefficient Closure_Effect_Direction\n",
      "                        Popular for Solo dining        0.443764         -0.464500 ↓ Strong Survival Driver\n",
      "                        Service options Takeout        0.119859         -0.310330 ↓ Strong Survival Driver\n",
      "                                 num of reviews        0.086911         -0.557761 ↓ Strong Survival Driver\n",
      "                              Atmosphere Casual        0.053423         -0.450025 ↓ Strong Survival Driver\n",
      "                        Amenities Good for kids        0.035284          0.409087  ↑ Strong Failure Driver\n",
      "                        Service options Dine-in        0.030336         -0.505792 ↓ Strong Survival Driver\n",
      "                         Offerings Comfort food        0.030047          0.019125    ↑ Weak Failure Driver\n",
      "   Accessibility Wheelchair accessible entrance        0.030043         -0.310831 ↓ Strong Survival Driver\n",
      "                                  price numeric        0.019744          0.027065    ↑ Weak Failure Driver\n",
      "                  Planning Accepts reservations        0.016096          0.132987    ↑ Weak Failure Driver\n",
      "                           Payments Debit cards        0.014671          0.171779  ↑ Strong Failure Driver\n",
      "                           Offerings Quick bite        0.012362         -0.461090 ↓ Strong Survival Driver\n",
      "                   Payments NFC mobile payments        0.011423         -0.325922 ↓ Strong Survival Driver\n",
      "                                     avg rating        0.011250         -0.032726   ↓ Weak Survival Driver\n",
      "                           Dining options Lunch        0.007994          0.137703    ↑ Weak Failure Driver\n",
      "                          Dining options Dinner        0.007883          0.171593  ↑ Strong Failure Driver\n",
      "                       Service options Delivery        0.007092          0.274918  ↑ Strong Failure Driver\n",
      "                             Amenities Restroom        0.006583          0.007097   Neutral / Not Selected\n",
      "                                   Crowd Groups        0.006291         -0.029929   ↓ Weak Survival Driver\n",
      "                       irs estimated population        0.005713          0.000763   Neutral / Not Selected\n",
      "                          Payments Credit cards        0.004909         -0.179192 ↓ Strong Survival Driver\n",
      "   Accessibility Wheelchair accessible restroom        0.004556         -0.108786   ↓ Weak Survival Driver\n",
      "                                Atmosphere Cozy        0.003779         -0.104429   ↓ Weak Survival Driver\n",
      "                             Popular for Dinner        0.003743         -0.117191   ↓ Weak Survival Driver\n",
      "                          Crowd Family-friendly        0.003356         -0.037858   ↓ Weak Survival Driver\n",
      "                               shrunk sentiment        0.003177          0.031346    ↑ Weak Failure Driver\n",
      "                               Offerings Coffee        0.003172          0.031168    ↑ Weak Failure Driver\n",
      "                         Dining options Dessert        0.002949          0.000000   Neutral / Not Selected\n",
      "                              Popular for Lunch        0.002940         -0.012268   ↓ Weak Survival Driver\n",
      "                          Amenities High chairs        0.002229          0.000000   Neutral / Not Selected\n",
      "                            Atmosphere Romantic        0.002091          0.000000   Neutral / Not Selected\n",
      "Accessibility Wheelchair accessible parking lot        0.001482         -0.035583   ↓ Weak Survival Driver\n",
      "       Planning Dinner reservations recommended        0.001411         -0.040594   ↓ Weak Survival Driver\n",
      "                                 Crowd Tourists        0.000917         -0.229168 ↓ Strong Survival Driver\n",
      "                                  sentiment std        0.000703          0.000000   Neutral / Not Selected\n",
      "                 Highlights Great tea selection        0.000563          0.000000   Neutral / Not Selected\n",
      "                        Highlights Fast service        0.000551         -0.031510   ↓ Weak Survival Driver\n",
      "                        Planning LGBTQ friendly        0.000467         -0.028583   ↓ Weak Survival Driver\n",
      "                        Highlights Great coffee        0.000237          0.000000   Neutral / Not Selected\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# ============================================================\n",
    "# 1. Prepare features for Elastic Net Logistic Regression (Multicollinearity between Features)\n",
    "# ============================================================\n",
    "\n",
    "assembler_enet = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_enet\"\n",
    ")\n",
    "\n",
    "# Remove old features_enet column if it exists\n",
    "if \"features_enet\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"features_enet\")\n",
    "if \"features_enet\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"features_enet\")\n",
    "\n",
    "train_df_enet = assembler_enet.transform(train_df)\n",
    "test_df_enet = assembler_enet.transform(test_df)\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler(inputCol=\"features_enet\", outputCol=\"features_scaled\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_df_enet)\n",
    "train_df_scaled = scaler_model.transform(train_df_enet)\n",
    "test_df_scaled = scaler_model.transform(test_df_enet)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Train Elastic Net Logistic Regression (for direction)\n",
    "# ============================================================\n",
    "\n",
    "elastic_lr = LogisticRegression(\n",
    "    featuresCol=\"features_scaled\",\n",
    "    labelCol=\"permanent_closed\",\n",
    "    elasticNetParam=0.2,   # Elastic Net (mix of Lasso & Ridge)\n",
    "    regParam=0.01,\n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "elastic_model = elastic_lr.fit(train_df_scaled)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Extract coefficients with direction\n",
    "# ============================================================\n",
    "\n",
    "elastic_coef = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"ENet_Coefficient\": elastic_model.coefficients.toArray()\n",
    "})\n",
    "elastic_coef[\"abs_coef\"] = elastic_coef[\"ENet_Coefficient\"].abs()\n",
    "\n",
    "# ============================================================\n",
    "# 4. Gradient-Boosted Trees feature importance\n",
    "# ============================================================\n",
    "\n",
    "gbt_importances = gbt_model.featureImportances.toArray()\n",
    "if len(gbt_importances) != len(feature_cols):\n",
    "    usable_cols = feature_cols[:len(gbt_importances)]\n",
    "else:\n",
    "    usable_cols = feature_cols\n",
    "\n",
    "feature_importance_gbt = pd.DataFrame({\n",
    "    \"feature\": usable_cols,\n",
    "    \"GBT_Importance\": gbt_importances\n",
    "}).sort_values(\"GBT_Importance\", ascending=False)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Merge GBT importance with Elastic Net coefficients\n",
    "# ============================================================\n",
    "\n",
    "final_df = feature_importance_gbt.merge(\n",
    "    elastic_coef,\n",
    "    on=\"feature\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Determine effect direction\n",
    "# ============================================================\n",
    "\n",
    "def determine_effect(coef):\n",
    "    if pd.isna(coef) or abs(coef) < 0.01:\n",
    "        return \"Neutral / Not Selected\"\n",
    "    elif coef > 0.15:\n",
    "        return \"↑ Strong Failure Driver\"\n",
    "    elif coef > 0:\n",
    "        return \"↑ Weak Failure Driver\"\n",
    "    elif coef < -0.15:\n",
    "        return \"↓ Strong Survival Driver\"\n",
    "    else:\n",
    "        return \"↓ Weak Survival Driver\"\n",
    "\n",
    "final_df[\"Closure_Effect_Direction\"] = final_df[\"ENet_Coefficient\"].apply(determine_effect)\n",
    "\n",
    "# ============================================================\n",
    "# 7. Select top N features by GBT importance\n",
    "# ============================================================\n",
    "\n",
    "top_n = 50\n",
    "top_features = final_df.head(top_n).copy()\n",
    "\n",
    "# ============================================================\n",
    "# 8. Clean feature names for presentation\n",
    "# ============================================================\n",
    "\n",
    "top_features[\"Feature_Clean\"] = (\n",
    "    top_features[\"feature\"]\n",
    "    .str.replace(\"MISC_\", \"\", regex=False)\n",
    "    .str.replace(\"_flag\", \"\", regex=False)\n",
    "    .str.replace(\"_\", \" \", regex=False)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 9. Final presentation table\n",
    "# ============================================================\n",
    "\n",
    "final_cols = [\n",
    "    \"Feature_Clean\",\n",
    "    \"GBT_Importance\",\n",
    "    \"ENet_Coefficient\",\n",
    "    \"Closure_Effect_Direction\"\n",
    "]\n",
    "\n",
    "print(\"\\n#############################################################\")\n",
    "print(\"###  Top Predictors with GBT Importance and Direction ###\")\n",
    "print(\"#############################################################\\n\")\n",
    "\n",
    "print(top_features[final_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a637f40-c041-4ba1-9b04-695294b46c1a",
   "metadata": {},
   "source": [
    "## 4. Predict Store Closures for All Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52be3940-ffb5-4d00-a7e2-285d5dfb6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------+----------+--------------------+\n",
      "|gmap_id                              |store_name                     |prediction|closure_prob        |\n",
      "+-------------------------------------+-------------------------------+----------+--------------------+\n",
      "|0x89e85a90fa80ea79:0x2f56cdd1f58118f |Checkers                       |0.0       |0.007217116418191323|\n",
      "|0x89c2c219189e5189:0x684a238fa71eb176|Momo Asian Fusion              |0.0       |0.017988535282472868|\n",
      "|0x8085b6a8ebae49f1:0x9f24861c2d643f9a|Rosso Pizzeria & Mozzarella Bar|1.0       |0.9591585341045397  |\n",
      "|0x89c258545813c6bf:0x8ee1343834123591|Junior's Restaurant & Bakery   |1.0       |0.5504999064554688  |\n",
      "|0x880fc80bb9076013:0xf73fd28c4d61b7ae|Woori Village                  |0.0       |0.18938758140253537 |\n",
      "|0x80dce7e4827a029f:0xcb9497eb98076b9d|The Flame Broiler              |0.0       |0.05205610831563328 |\n",
      "|0x89c25baa8b6fb9c9:0x5c4f2f36d6850943|200 Fifth                      |1.0       |0.7581311727980395  |\n",
      "|0x89d0b747c5aaf579:0xd3ea18ab8ff58c3c|Wolffy's Grill and Marina      |0.0       |0.012345089241641037|\n",
      "|0x80dcd89c71f46e37:0x419d4c74a1fbfd1a|San Salvador Restaurant        |0.0       |0.004609386299243671|\n",
      "|0x89c292f56593548d:0x46bb4c98b1e2e015|La Herradura Inc II            |1.0       |0.5751841549502201  |\n",
      "+-------------------------------------+-------------------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1️⃣ Transform all data\n",
    "all_predictions = cv_model_rf.bestModel.transform(business_df_clean)\n",
    "\n",
    "# 2️⃣ Convert SparseVector probability to array\n",
    "all_predictions = all_predictions.withColumn(\n",
    "    \"prob_array\",\n",
    "    vector_to_array(col(\"business_closure_prob\"))\n",
    ")\n",
    "\n",
    "# 3️⃣ Extract the probability of closure (class 1)\n",
    "all_predictions = all_predictions.withColumn(\n",
    "    \"closure_prob\",\n",
    "    col(\"prob_array\")[1]  # now safe\n",
    ")\n",
    "\n",
    "# 4️⃣ Select the final columns\n",
    "biz_closure_prob_df = all_predictions.select(\n",
    "    \"gmap_id\",\n",
    "    \"store_name\",\n",
    "    \"prediction\",\n",
    "    \"closure_prob\"\n",
    ")\n",
    "\n",
    "# 5️⃣ Show results\n",
    "biz_closure_prob_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaddcc5-beec-4d79-ac94-f28f3db8d874",
   "metadata": {},
   "source": [
    "####  Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1638d85-40cd-4d9f-8c3a-5e824bb1bece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "biz_closure_prob_df.write.mode(\"overwrite\").parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/biz_closure_prob_df/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7b951-b88e-4167-8fda-0292a18c774d",
   "metadata": {},
   "source": [
    "## 5. EDA on NLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf17966-2120-424f-9853-a98ca9252105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        avg_rating|\n",
      "+-------+------------------+\n",
      "|  count|            169478|\n",
      "|   mean| 4.199176294268276|\n",
      "| stddev|0.4117197819420847|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6838:=======>                                                (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|rating_bin|count|\n",
      "+----------+-----+\n",
      "|     1.5-2|   42|\n",
      "|     2-2.5|  486|\n",
      "|     2.5-3| 2874|\n",
      "|     3-3.5|12928|\n",
      "|     3.5-4|40745|\n",
      "|     4-4.5|75402|\n",
      "|     4.5-5|37001|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, round\n",
    "\n",
    "# Basic statistics\n",
    "business_df.select(\"avg_rating\").describe().show()\n",
    "\n",
    "\n",
    "# Optional: Bin ratings into ranges (e.g., 1-2, 2-3, 3-4, 4-5)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "business_df = business_df.withColumn(\n",
    "    \"rating_bin\",\n",
    "    when(col(\"avg_predicted_sentiment\") < 1.5, \"1-1.5\") \\\n",
    "    .when(col(\"avg_predicted_sentiment\") < 2, \"1.5-2\") \\\n",
    "    .when(col(\"avg_predicted_sentiment\") < 2.5, \"2-2.5\") \\\n",
    "    .when(col(\"avg_predicted_sentiment\") < 3, \"2.5-3\") \\\n",
    "    .when(col(\"avg_predicted_sentiment\") < 3.5, \"3-3.5\") \\\n",
    "    .when(col(\"avg_predicted_sentiment\") < 4, \"3.5-4\") \\\n",
    "    .when(col(\"avg_predicted_sentiment\") < 4.5, \"4-4.5\") \\\n",
    "    .otherwise(\"4.5-5\")\n",
    ")\n",
    "\n",
    "\n",
    "business_df.groupBy(\"rating_bin\").agg(count(\"*\").alias(\"count\")) \\\n",
    "               .orderBy(\"rating_bin\") \\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3826d6-1793-4324-8eb4-4beda6eb84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------+----+--------------------+-------------+--------------------+--------------------+----------+--------------------+-----------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+------------------+------------------+---------------+---------------+---------------+--------------------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+-------------+-------------+--------------------+--------------+--------------------+-----+--------+---------+-------------+------------------------+\n",
      "|             gmap_id|     cust_name|rating|resp|                text|         time|             user_id|             address|avg_rating|            category|description|               hours|          latitude|         longitude|          store_name|num_of_reviews|price|    relative_results|             state|MISC_Accessibility|MISC_Activities| MISC_Amenities|MISC_Atmosphere|          MISC_Crowd|MISC_Dining_options|MISC_From_the_business|MISC_Getting_here|MISC_Health_and_safety|MISC_Highlights|MISC_Lodging_options|      MISC_Offerings|MISC_Payments|MISC_Planning|    MISC_Popular_for|MISC_Recycling|MISC_Service_options|  zip|zip_city|zip_state|   zip_county|irs_estimated_population|\n",
      "+--------------------+--------------+------+----+--------------------+-------------+--------------------+--------------------+----------+--------------------+-----------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+------------------+------------------+---------------+---------------+---------------+--------------------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+-------------+-------------+--------------------+--------------+--------------------+-----+--------+---------+-------------+------------------------+\n",
      "|0x4065fd476208a27...|Chiderah Abani|     5|null|Love, love, love....|1528890236792|11208554345809883...|New York Kim-Bob ...|       4.3|[Korean restauran...|       null|[[Monday, 7AM–9PM...|40.764643199999995|-73.81192399999999|New York Kim-Bob ...|            68|    $|[0x89c2602e5c9513...|Closed ⋅ Opens 7AM|              null|           null|[Good for kids]| [Casual, Cozy]|[College students...|               null|                  null|             null|                  null|           null|                null|[Comfort food, He...|         null|         null|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|11354|Flushing|       NY|Queens County|                   51190|\n",
      "|0x4065fd476208a27...|  luis cabrera|     5|null|My favorite Kimba...|1589581105858|11034451447099652...|New York Kim-Bob ...|       4.3|[Korean restauran...|       null|[[Monday, 7AM–9PM...|40.764643199999995|-73.81192399999999|New York Kim-Bob ...|            68|    $|[0x89c2602e5c9513...|Closed ⋅ Opens 7AM|              null|           null|[Good for kids]| [Casual, Cozy]|[College students...|               null|                  null|             null|                  null|           null|                null|[Comfort food, He...|         null|         null|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|11354|Flushing|       NY|Queens County|                   51190|\n",
      "|0x4065fd476208a27...|     SUNG CHOI|     5|null|                null|1485139260721|10431700601085267...|New York Kim-Bob ...|       4.3|[Korean restauran...|       null|[[Monday, 7AM–9PM...|40.764643199999995|-73.81192399999999|New York Kim-Bob ...|            68|    $|[0x89c2602e5c9513...|Closed ⋅ Opens 7AM|              null|           null|[Good for kids]| [Casual, Cozy]|[College students...|               null|                  null|             null|                  null|           null|                null|[Comfort food, He...|         null|         null|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|11354|Flushing|       NY|Queens County|                   51190|\n",
      "+--------------------+--------------+------+----+--------------------+-------------+--------------------+--------------------+----------+--------------------+-----------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+------------------+------------------+---------------+---------------+---------------+--------------------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+-------------+-------------+--------------------+--------------+--------------------+-----+--------+---------+-------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/combined_rest_df/\") \n",
    "combined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac4e4cc-a9d8-4552-b5a5-756eb5c91e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "combined_df = combined_df.withColumn(\n",
    "    \"permanent_closed\",\n",
    "    when(col(\"state\").like(\"%Permanently closed%\"), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b66a2c-f08a-472d-93be-a13e0299f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845a9f-5369-46d8-823d-c8e022294467",
   "metadata": {},
   "source": [
    "#### Most Common Tokens for Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7040cee7-189e-418f-9593-ddddc5d9dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6858:====================================================> (43 + 1) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|token     |count |\n",
      "+----------+------+\n",
      "|food      |431697|\n",
      "|great     |330406|\n",
      "|good      |305473|\n",
      "|place     |203824|\n",
      "|service   |173118|\n",
      "|like      |99338 |\n",
      "|best      |98113 |\n",
      "|nice      |92601 |\n",
      "|really    |85097 |\n",
      "|love      |84047 |\n",
      "|go        |81500 |\n",
      "|one       |78695 |\n",
      "|get       |76827 |\n",
      "|friendly  |75135 |\n",
      "|staff     |73305 |\n",
      "|time      |69861 |\n",
      "|always    |65960 |\n",
      "|delicious |64934 |\n",
      "|back      |61985 |\n",
      "|restaurant|60099 |\n",
      "|chicken   |58369 |\n",
      "|excellent |53132 |\n",
      "|pizza     |52727 |\n",
      "|order     |51318 |\n",
      "|also      |50503 |\n",
      "|even      |49435 |\n",
      "|little    |48366 |\n",
      "|ordered   |48013 |\n",
      "|definitely|47579 |\n",
      "|got       |47344 |\n",
      "|amazing   |45921 |\n",
      "|never     |45329 |\n",
      "|eat       |43611 |\n",
      "|come      |43357 |\n",
      "|try       |42644 |\n",
      "|recommend |41082 |\n",
      "|us        |40773 |\n",
      "|fresh     |39197 |\n",
      "|came      |38897 |\n",
      "|menu      |38123 |\n",
      "|went      |36914 |\n",
      "|people    |35815 |\n",
      "|first     |33765 |\n",
      "|pretty    |33491 |\n",
      "|well      |33152 |\n",
      "|made      |33033 |\n",
      "|awesome   |32877 |\n",
      "|ever      |32212 |\n",
      "|make      |32178 |\n",
      "|everything|31872 |\n",
      "+----------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter reviews for closed stores\n",
    "closed_reviews = english_df.filter(F.col(\"permanent_closed\") == 1)\n",
    "\n",
    "# Explode english_tokens into separate rows\n",
    "closed_tokens = closed_reviews.select(F.explode(F.col(\"english_tokens\")).alias(\"token\"))\n",
    "\n",
    "# Count token frequency\n",
    "closed_token_counts = closed_tokens.groupBy(\"token\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Show top 20 most common tokens\n",
    "closed_token_counts.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53e803-d045-4053-9653-103910064c67",
   "metadata": {},
   "source": [
    "#### Remove Store Name Top N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3dc63bb-911e-4e2e-8aa6-a391ee6b6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Keep only reviews from permanently closed stores\n",
    "# ----------------------------\n",
    "closed_df = combined_df.filter(\n",
    "    (F.col(\"text\").isNotNull()) &\n",
    "    (F.col(\"rating\").isNotNull()) &\n",
    "    (F.col(\"permanent_closed\") == 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e56eae1-1bce-469a-86d5-bf829283ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1188577"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7eeb0-b7d4-4869-b47d-f2f9d2f024d4",
   "metadata": {},
   "source": [
    "#### Top Trigrams Closed Restaurants¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3780463-e6af-4772-8b3b-ad73ed224e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6887:===================================================>  (42 + 2) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+\n",
      "|ngram                        |closed_count|\n",
      "+-----------------------------+------------+\n",
      "|highly recommend anyone      |376         |\n",
      "|definitely recommend anyone  |264         |\n",
      "|highly recommend try         |208         |\n",
      "|decided give try             |199         |\n",
      "|ordered took minutes         |179         |\n",
      "|highly recommend trying      |168         |\n",
      "|highly recommend going       |165         |\n",
      "|customer highly recommend    |157         |\n",
      "|definitely highly recommend  |145         |\n",
      "|recommend anyone wants       |143         |\n",
      "|highly recommend definitely  |128         |\n",
      "|loved highly recommend       |124         |\n",
      "|took almost minutes          |103         |\n",
      "|highly recommend checking    |102         |\n",
      "|definitely recommend trying  |100         |\n",
      "|definitely recommend try     |98          |\n",
      "|definitely going try         |96          |\n",
      "|waited minutes waitress      |93          |\n",
      "|try highly recommend         |92          |\n",
      "|took minutes bring           |91          |\n",
      "|reasonable highly recommend  |91          |\n",
      "|ordered waited minutes       |89          |\n",
      "|definitely recommend going   |89          |\n",
      "|took minutes ordered         |88          |\n",
      "|(translated google)          |87          |\n",
      "|waited almost minutes        |87          |\n",
      "|tried highly recommend       |86          |\n",
      "|highly recommend stopping    |85          |\n",
      "|asked waitress said          |85          |\n",
      "|enjoyed highly recommend     |83          |\n",
      "|loved definitely recommend   |80          |\n",
      "|ordered ordered ordered      |80          |\n",
      "|going highly recommend       |80          |\n",
      "|ordered highly recommend     |79          |\n",
      "|asked waiter said            |78          |\n",
      "|waited minutes someone       |78          |\n",
      "|took minutes waitress        |76          |\n",
      "|highly recommend give        |76          |\n",
      "|took minutes minutes         |75          |\n",
      "|said asked said              |75          |\n",
      "|waited minutes waiter        |75          |\n",
      "|asked said asked             |74          |\n",
      "|asked manager said           |74          |\n",
      "|                             |73          |\n",
      "|recommend anyone try         |72          |\n",
      "|customer definitely recommend|68          |\n",
      "|went waited minutes          |68          |\n",
      "|decided try ordered          |68          |\n",
      "|ordered husband ordered      |68          |\n",
      "|said said said               |68          |\n",
      "+-----------------------------+------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Keep only reviews from permanently closed stores\n",
    "# ----------------------------\n",
    "closed_df = combined_df.filter(\n",
    "    (F.col(\"text\").isNotNull()) &\n",
    "    (F.col(\"rating\").isNotNull()) &\n",
    "    (F.col(\"permanent_closed\") == 1)\n",
    ")\n",
    "\n",
    "# sample 1% for testing\n",
    "closed_df = closed_df.sample(fraction=1.0, seed=42)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Tokenize\n",
    "# ----------------------------\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "closed_df = tokenizer.transform(closed_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Remove stopwords\n",
    "# ----------------------------\n",
    "stop_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
    "closed_df = stop_remover.transform(closed_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Keep only alphabetic English tokens >= 3 chars\n",
    "# ----------------------------\n",
    "closed_df = closed_df.withColumn(\n",
    "    \"english_tokens\",\n",
    "    F.expr(\"filter(transform(tokens_clean, x -> lower(x)), x -> x rlike '^[a-z]{3,}$')\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Remove store name words using broadcast UDF\n",
    "# ----------------------------\n",
    "store_words_list = (\n",
    "    combined_df\n",
    "    .select(F.lower(F.col(\"store_name\")).alias(\"name\"))\n",
    "    .select(F.split(\"name\", \" \").alias(\"words\"))\n",
    "    .select(F.explode(\"words\").alias(\"w\"))\n",
    "    .filter(F.length(\"w\") > 2)\n",
    "    .distinct()\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "broadcast_store_words = spark.sparkContext.broadcast(set(store_words_list))\n",
    "\n",
    "def remove_store_words(tokens):\n",
    "    return [t for t in tokens if t not in broadcast_store_words.value]\n",
    "\n",
    "remove_store_words_udf = F.udf(remove_store_words, ArrayType(StringType()))\n",
    "\n",
    "closed_df = closed_df.withColumn(\"operational_tokens\", remove_store_words_udf(\"english_tokens\"))\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Create trigrams\n",
    "# ----------------------------\n",
    "trigrammer = NGram(n=3, inputCol=\"operational_tokens\", outputCol=\"trigrams\")\n",
    "closed_df = trigrammer.transform(closed_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Explode trigrams and count\n",
    "# ----------------------------\n",
    "trigram_exploded = closed_df.select(F.explode(\"trigrams\").alias(\"ngram\"))\n",
    "trigram_counts = trigram_exploded.groupBy(\"ngram\").count().withColumnRenamed(\"count\", \"closed_count\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Rank trigrams by frequency\n",
    "# ----------------------------\n",
    "trigram_stats = trigram_counts.orderBy(F.col(\"closed_count\").desc())\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Show top 50 trigrams\n",
    "# ----------------------------\n",
    "trigram_stats.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f96d21-a9a0-42e4-8a4e-f2ac37d3b453",
   "metadata": {},
   "source": [
    "#### Comparative Trigram Analysis for Closed vs. Open Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12c07c61-1aea-4e43-8857-8ba919355edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams indicative of closed stores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------+----------+------------------+\n",
      "|ngram                          |closed_count|open_count|log_odds_ratio    |\n",
      "+-------------------------------+------------+----------+------------------+\n",
      "|scammers scammers scammers     |55          |0         |4.02535169073515  |\n",
      "|fluffy fluffy fluffy           |17          |0         |2.8903717578961645|\n",
      "|uuuuu uuuuu uuuuu              |16          |0         |2.833213344056216 |\n",
      "|spend courteous recommend      |5           |0         |1.791759469228055 |\n",
      "|gambar pertama adalah          |4           |0         |1.6094379124341003|\n",
      "|particular recommend arugula   |4           |0         |1.6094379124341003|\n",
      "|bathroom toilet prosper        |4           |0         |1.6094379124341003|\n",
      "|generous alcohol ordering      |4           |0         |1.6094379124341003|\n",
      "|recommend comparison many      |4           |0         |1.6094379124341003|\n",
      "|ordered takes prepare          |4           |0         |1.6094379124341003|\n",
      "|steal putting wallet           |4           |0         |1.6094379124341003|\n",
      "|ordering issue fairly          |4           |0         |1.6094379124341003|\n",
      "|said cheating expensive        |4           |0         |1.6094379124341003|\n",
      "|speaks generous alcohol        |4           |0         |1.6094379124341003|\n",
      "|waiter groupon coupon          |4           |0         |1.6094379124341003|\n",
      "|toilet prosper succeed         |4           |0         |1.6094379124341003|\n",
      "|called waitress went           |4           |0         |1.6094379124341003|\n",
      "|unexpected professional decor  |4           |0         |1.6094379124341003|\n",
      "|called said cheating           |4           |0         |1.6094379124341003|\n",
      "|nymph cockroaches crawling     |4           |0         |1.6094379124341003|\n",
      "|going steal putting            |4           |0         |1.6094379124341003|\n",
      "|tried months try               |4           |0         |1.6094379124341003|\n",
      "|subtleties noticed attention   |4           |0         |1.6094379124341003|\n",
      "|adalah facebook sebelum        |4           |0         |1.6094379124341003|\n",
      "|alcohol ordering issue         |4           |0         |1.6094379124341003|\n",
      "|ordered despite took           |4           |0         |1.6094379124341003|\n",
      "|dengan gambar pertama          |4           |0         |1.6094379124341003|\n",
      "|professional decor try         |4           |0         |1.6094379124341003|\n",
      "|clearly went wrong             |4           |0         |1.6094379124341003|\n",
      "|cheating expensive without     |4           |0         |1.6094379124341003|\n",
      "|attractive particular recommend|4           |0         |1.6094379124341003|\n",
      "|pertama adalah facebook        |4           |0         |1.6094379124341003|\n",
      "|ketiga dengan gambar           |4           |0         |1.6094379124341003|\n",
      "|changes personal lighting      |4           |0         |1.6094379124341003|\n",
      "|muncha muncha muncha           |4           |0         |1.6094379124341003|\n",
      "|mention recommend almost       |4           |0         |1.6094379124341003|\n",
      "|soupy mostly almost            |3           |0         |1.3862943611198906|\n",
      "|react said complete            |3           |0         |1.3862943611198906|\n",
      "|gave photos lying              |3           |0         |1.3862943611198906|\n",
      "|excited walked server          |3           |0         |1.3862943611198906|\n",
      "|broader least considering      |3           |0         |1.3862943611198906|\n",
      "|definitely tried groupon       |3           |0         |1.3862943611198906|\n",
      "|either many types              |3           |0         |1.3862943611198906|\n",
      "|piyo aish karo                 |3           |0         |1.3862943611198906|\n",
      "|exterior emanating smells      |3           |0         |1.3862943611198906|\n",
      "|shout customers find           |3           |0         |1.3862943611198906|\n",
      "|moderately accept appetizers   |3           |0         |1.3862943611198906|\n",
      "|significant majority vhs       |3           |0         |1.3862943611198906|\n",
      "|discovered layering filled     |3           |0         |1.3862943611198906|\n",
      "|expected wait looked           |3           |0         |1.3862943611198906|\n",
      "+-------------------------------+------------+----------+------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "Trigrams indicative of open stores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6915:==============================================>       (39 + 6) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------+----------+-------------------+\n",
      "|ngram                            |closed_count|open_count|log_odds_ratio     |\n",
      "+---------------------------------+------------+----------+-------------------+\n",
      "|lolol lolol lolol                |0           |363       |-5.8971538676367405|\n",
      "|frickin frickin frickin          |0           |220       |-5.3981627015177525|\n",
      "|wearing masks gloves             |0           |192       |-5.262690188904886 |\n",
      "|worst customer wait              |0           |177       |-5.181783550292085 |\n",
      "|wait waited almost               |0           |163       |-5.099866427824199 |\n",
      "|wrong missing items              |0           |160       |-5.081404364984463 |\n",
      "|took minutes cars                |1           |298       |-5.0072963928307415|\n",
      "|gave someone wait                |0           |148       |-5.003946305945459 |\n",
      "|covid highly recommend           |0           |139       |-4.941642422609305 |\n",
      "|rude customer skills             |0           |136       |-4.919980925828125 |\n",
      "|ordered asked wait               |0           |135       |-4.912654885736052 |\n",
      "|none wearing masks               |0           |131       |-4.882801922586371 |\n",
      "|loved members highly             |0           |129       |-4.867534450455582 |\n",
      "|feedback efficient whenever      |0           |129       |-4.867534450455582 |\n",
      "|cashier rude asked               |0           |127       |-4.852030263919617 |\n",
      "|following covid guidelines       |0           |125       |-4.836281906951478 |\n",
      "|someone taking orders            |0           |121       |-4.804021044733257 |\n",
      "|reviews excited try              |0           |120       |-4.795790545596741 |\n",
      "|covid safety measures            |0           |119       |-4.787491742782046 |\n",
      "|ordered paid received            |0           |118       |-4.77912349311153  |\n",
      "|customers wearing masks          |0           |118       |-4.77912349311153  |\n",
      "|typical highly recommend         |0           |117       |-4.770684624465665 |\n",
      "|wearing mask properly            |0           |116       |-4.762173934797756 |\n",
      "|minutes cars ahead               |3           |460       |-4.747103681876758 |\n",
      "|paying attention customer        |0           |112       |-4.727387818712341 |\n",
      "|customer rates convenient        |0           |112       |-4.727387818712341 |\n",
      "|cars waited minutes              |0           |110       |-4.709530201312334 |\n",
      "|wait approximately minutes       |0           |109       |-4.700480365792417 |\n",
      "|wearing masks properly           |0           |108       |-4.6913478822291435|\n",
      "|forgot give gave                 |0           |105       |-4.663439094112067 |\n",
      "|wrong ordered gave               |0           |104       |-4.653960350157523 |\n",
      "|wait seated definitely           |0           |104       |-4.653960350157523 |\n",
      "|topmost doubt warmly             |0           |103       |-4.6443908991413725|\n",
      "|ordered given wrong              |0           |102       |-4.634728988229636 |\n",
      "|orders missing items             |0           |101       |-4.624972813284271 |\n",
      "|wait went waited                 |0           |100       |-4.61512051684126  |\n",
      "|wearing mask wearing             |0           |100       |-4.61512051684126  |\n",
      "|wait try wait                    |0           |100       |-4.61512051684126  |\n",
      "|favourite places arrived         |0           |99        |-4.605170185988091 |\n",
      "|minute wait cars                 |0           |98        |-4.59511985013459  |\n",
      "|sometimes forget give            |0           |98        |-4.59511985013459  |\n",
      "|offers loved rates               |0           |97        |-4.584967478670572 |\n",
      "|manager horrible customer        |0           |95        |-4.564348191467836 |\n",
      "|boner boner boner                |0           |94        |-4.553876891600541 |\n",
      "|surely recommend customer        |0           |94        |-4.553876891600541 |\n",
      "|gave asked asked                 |0           |94        |-4.553876891600541 |\n",
      "|many homeless hanging            |1           |186       |-4.537961436294641 |\n",
      "|customer mal cliente             |0           |92        |-4.532599493153256 |\n",
      "|definitely wait definitely       |0           |92        |-4.532599493153256 |\n",
      "|conveniently regularly delightful|0           |91        |-4.5217885770490405|\n",
      "+---------------------------------+------------+----------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Filter closed and open store reviews\n",
    "# ----------------------------\n",
    "closed_df = combined_df.filter(\n",
    "    (F.col(\"text\").isNotNull()) &\n",
    "    (F.col(\"rating\").isNotNull()) &\n",
    "    (F.col(\"permanent_closed\") == 1)\n",
    ")\n",
    "\n",
    "open_df = combined_df.filter(\n",
    "    (F.col(\"text\").isNotNull()) &\n",
    "    (F.col(\"rating\").isNotNull()) &\n",
    "    (F.col(\"permanent_closed\") == 0)\n",
    ")\n",
    "\n",
    "# Optional: sample 1% for testing\n",
    "closed_df = closed_df.sample(1.0, seed=42)\n",
    "open_df   = open_df.sample(1.0, seed=42)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Tokenize & clean\n",
    "# ----------------------------\n",
    "def clean_tokens(df):\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    df = tokenizer.transform(df)\n",
    "\n",
    "    stop_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
    "    df = stop_remover.transform(df)\n",
    "\n",
    "    # Keep only alphabetic tokens of length >=3\n",
    "    df = df.withColumn(\n",
    "        \"english_tokens\",\n",
    "        F.expr(\"filter(transform(tokens_clean, x -> lower(x)), x -> x rlike '^[a-z]{3,}$')\")\n",
    "    )\n",
    "\n",
    "    # Filter out nonsense tokens like repeated letters\n",
    "    df = df.withColumn(\n",
    "        \"english_tokens\",\n",
    "        F.expr(\"filter(english_tokens, x -> not x rlike '^(.)\\\\1+$')\")  # removes 'aaaa' or 'uuu'\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "closed_df = clean_tokens(closed_df)\n",
    "open_df   = clean_tokens(open_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Remove store name words\n",
    "# ----------------------------\n",
    "store_words_list = (\n",
    "    combined_df\n",
    "    .select(F.lower(F.col(\"store_name\")).alias(\"name\"))\n",
    "    .select(F.split(\"name\", \" \").alias(\"words\"))\n",
    "    .select(F.explode(\"words\").alias(\"w\"))\n",
    "    .filter(F.length(\"w\") > 2)\n",
    "    .distinct()\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "broadcast_store_words = spark.sparkContext.broadcast(set(store_words_list))\n",
    "\n",
    "def remove_store_words(tokens):\n",
    "    return [t for t in tokens if t not in broadcast_store_words.value]\n",
    "\n",
    "remove_store_words_udf = F.udf(remove_store_words, ArrayType(StringType()))\n",
    "\n",
    "closed_df = closed_df.withColumn(\"operational_tokens\", remove_store_words_udf(\"english_tokens\"))\n",
    "open_df   = open_df.withColumn(\"operational_tokens\", remove_store_words_udf(\"english_tokens\"))\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Create trigrams\n",
    "# ----------------------------\n",
    "trigrammer = NGram(n=3, inputCol=\"operational_tokens\", outputCol=\"trigrams\")\n",
    "closed_df = trigrammer.transform(closed_df)\n",
    "open_df   = trigrammer.transform(open_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Explode trigrams and count\n",
    "# ----------------------------\n",
    "def trigram_count(df, count_col):\n",
    "    exploded = df.select(F.explode(\"trigrams\").alias(\"ngram\"))\n",
    "    return exploded.groupBy(\"ngram\").count().withColumnRenamed(\"count\", count_col)\n",
    "\n",
    "closed_counts = trigram_count(closed_df, \"closed_count\")\n",
    "open_counts   = trigram_count(open_df, \"open_count\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Combine counts & compute log-odds\n",
    "# ----------------------------\n",
    "combined_counts = closed_counts.join(open_counts, on=\"ngram\", how=\"outer\").fillna(0)\n",
    "combined_counts = combined_counts.withColumn(\n",
    "    \"log_odds_ratio\",\n",
    "    F.log((F.col(\"closed_count\")+1)/(F.col(\"open_count\")+1))\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Rank for comparative insight\n",
    "# ----------------------------\n",
    "top_closed_trigrams = combined_counts.orderBy(F.col(\"log_odds_ratio\").desc())\n",
    "top_open_trigrams   = combined_counts.orderBy(F.col(\"log_odds_ratio\").asc())\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Show top results\n",
    "# ----------------------------\n",
    "print(\"Trigrams indicative of closed stores:\")\n",
    "top_closed_trigrams.show(50, truncate=False)\n",
    "\n",
    "print(\"Trigrams indicative of open stores:\")\n",
    "top_open_trigrams.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e4057-0062-4d35-b964-966bbd4a9b41",
   "metadata": {},
   "source": [
    "#### Comparative 4-Gram Analysis for Open vs. Closed Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ec30d78-e830-49b2-ba20-6e7e0cec7fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-grams indicative of closed stores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+------------+----------+------------------+\n",
      "|ngram                                    |closed_count|open_count|log_odds_ratio    |\n",
      "+-----------------------------------------+------------+----------+------------------+\n",
      "|scammers scammers scammers scammers      |61          |0         |4.127134385045092 |\n",
      "|fluffy fluffy fluffy fluffy              |16          |0         |2.833213344056216 |\n",
      "|uuuuu uuuuu uuuuu uuuuu                  |9           |0         |2.302585092994046 |\n",
      "|gambar pertama adalah facebook           |4           |0         |1.6094379124341003|\n",
      "|dengan gambar pertama adalah             |4           |0         |1.6094379124341003|\n",
      "|called said cheating expensive           |4           |0         |1.6094379124341003|\n",
      "|alcohol ordering issue fairly            |4           |0         |1.6094379124341003|\n",
      "|said cheating expensive without          |4           |0         |1.6094379124341003|\n",
      "|ketiga dengan gambar pertama             |4           |0         |1.6094379124341003|\n",
      "|generous alcohol ordering issue          |4           |0         |1.6094379124341003|\n",
      "|pertama adalah facebook sebelum          |4           |0         |1.6094379124341003|\n",
      "|give threatened numerous invited         |3           |0         |1.3862943611198906|\n",
      "|ticked asked bartender transfer          |3           |0         |1.3862943611198906|\n",
      "|outstanding subtleties noticed attention |3           |0         |1.3862943611198906|\n",
      "|credit along debit without               |3           |0         |1.3862943611198906|\n",
      "|asco disaster trays experiences          |3           |0         |1.3862943611198906|\n",
      "|enter completely forget carefully        |3           |0         |1.3862943611198906|\n",
      "|definitely type multiple interior        |3           |0         |1.3862943611198906|\n",
      "|quite certainly actually afterthought    |3           |0         |1.3862943611198906|\n",
      "|careful robby manager sick               |3           |0         |1.3862943611198906|\n",
      "|majority vhs tapes films                 |3           |0         |1.3862943611198906|\n",
      "|indeed denied vianni stated              |3           |0         |1.3862943611198906|\n",
      "|bathroom toilet prosper succeed          |3           |0         |1.3862943611198906|\n",
      "|recommendation opens already updated     |3           |0         |1.3862943611198906|\n",
      "|bring suggest trying brownie             |3           |0         |1.3862943611198906|\n",
      "|answer definitely places accessible      |3           |0         |1.3862943611198906|\n",
      "|respectful lack hygiene premises         |3           |0         |1.3862943611198906|\n",
      "|perhaps grows older kicks                |3           |0         |1.3862943611198906|\n",
      "|unexpected professional decor try        |3           |0         |1.3862943611198906|\n",
      "|spend stated uber instead                |3           |0         |1.3862943611198906|\n",
      "|week surprised find birthday             |3           |0         |1.3862943611198906|\n",
      "|wait minutes especially considering      |3           |0         |1.3862943611198906|\n",
      "|surprised find birthday seated           |3           |0         |1.3862943611198906|\n",
      "|waiter give password avoid               |3           |0         |1.3862943611198906|\n",
      "|speaks generous alcohol ordering         |3           |0         |1.3862943611198906|\n",
      "|manager lyn expressed concern            |3           |0         |1.3862943611198906|\n",
      "|went discovered chis previous            |3           |0         |1.3862943611198906|\n",
      "|waiter ignored questions though          |3           |0         |1.3862943611198906|\n",
      "|ignored questions though asked           |3           |0         |1.3862943611198906|\n",
      "|applauded sampler scallop presentation   |3           |0         |1.3862943611198906|\n",
      "|ordered week worse verbally              |3           |0         |1.3862943611198906|\n",
      "|manager engage fighting speaking         |3           |0         |1.3862943611198906|\n",
      "|cancelled thats review helps             |3           |0         |1.3862943611198906|\n",
      "|professional specially helpful especially|3           |0         |1.3862943611198906|\n",
      "|recently recommendations rundown exterior|3           |0         |1.3862943611198906|\n",
      "|find birthday seated ordered             |3           |0         |1.3862943611198906|\n",
      "|recently quite certainly actually        |3           |0         |1.3862943611198906|\n",
      "|detect liver quite cordial               |3           |0         |1.3862943611198906|\n",
      "|week decided leave manager               |3           |0         |1.3862943611198906|\n",
      "|mushrooms sense amount put               |3           |0         |1.3862943611198906|\n",
      "+-----------------------------------------+------------+----------+------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "4-grams indicative of open stores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6925:=============================================>        (39 + 7) / 46]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+------------+----------+-------------------+\n",
      "|ngram                                    |closed_count|open_count|log_odds_ratio     |\n",
      "+-----------------------------------------+------------+----------+-------------------+\n",
      "|lolol lolol lolol lolol                  |0           |262       |-5.572154032177765 |\n",
      "|frickin frickin frickin frickin          |0           |176       |-5.176149732573829 |\n",
      "|regularly charge convenient rates        |0           |117       |-4.770684624465665 |\n",
      "|comes instantly many types               |0           |95        |-4.564348191467836 |\n",
      "|boner boner boner boner                  |0           |93        |-4.543294782270004 |\n",
      "|trying consistently towards visitors     |1           |182       |-4.516338972281476 |\n",
      "|constantly charge reasonable rates       |0           |90        |-4.51085950651685  |\n",
      "|cars ahead took minutes                  |0           |90        |-4.51085950651685  |\n",
      "|surely recommend visiting customer       |0           |87        |-4.477336814478207 |\n",
      "|comes straightaway many types            |0           |86        |-4.465908118654584 |\n",
      "|seems needs observe customer             |0           |84        |-4.442651256490317 |\n",
      "|pacer meme deadthe pacer                 |0           |81        |-4.406719247264253 |\n",
      "|meme deadthe pacer meme                  |0           |80        |-4.394449154672439 |\n",
      "|rates efficient definitely recommend     |0           |79        |-4.382026634673881 |\n",
      "|deadthe pacer meme deadthe               |0           |79        |-4.382026634673881 |\n",
      "|waited minutes went ask                  |0           |77        |-4.356708826689592 |\n",
      "|members welcomed felt loved              |0           |75        |-4.330733340286331 |\n",
      "|suggests everytime comes straightaway    |0           |74        |-4.31748811353631  |\n",
      "|towards clients frequently highly        |0           |71        |-4.276666119016055 |\n",
      "|prefere opportunity whenever need        |0           |71        |-4.276666119016055 |\n",
      "|productive personal definitely recommend |0           |70        |-4.2626798770413155|\n",
      "|highest feedback efficient whenever      |0           |68        |-4.23410650459726  |\n",
      "|took minutes cars ahead                  |0           |68        |-4.23410650459726  |\n",
      "|portions affordable regularly organized  |0           |67        |-4.219507705176107 |\n",
      "|reasonable considerable portions members |0           |66        |-4.204692619390966 |\n",
      "|(translated google)  y                   |0           |66        |-4.204692619390966 |\n",
      "|called asked manager said                |0           |65        |-4.189654742026425 |\n",
      "|put shall surely highly                  |1           |128       |-4.1666652238017265|\n",
      "|everytime need decent comes              |0           |63        |-4.1588830833596715|\n",
      "|giving portions affordable consistently  |0           |61        |-4.127134385045092 |\n",
      "|highest recommendation generous whenever |0           |61        |-4.127134385045092 |\n",
      "|provide consistently towards customers   |0           |59        |-4.0943445622221   |\n",
      "|suggests whenever decent comes           |0           |59        |-4.0943445622221   |\n",
      "|portions affordable liked members        |0           |59        |-4.0943445622221   |\n",
      "|rates effective amiable highly           |0           |59        |-4.0943445622221   |\n",
      "|waited minutes cars ahead                |1           |117       |-4.07753744390572  |\n",
      "|highly recommend definitely returning    |0           |58        |-4.07753744390572  |\n",
      "|productive amiable highly recommend      |0           |57        |-4.060443010546419 |\n",
      "|warmest feedback efficient whenever      |0           |57        |-4.060443010546419 |\n",
      "|generous portions economical enjoyed     |0           |56        |-4.04305126783455  |\n",
      "|put shall doubt highly                   |1           |112       |-4.034240638152395 |\n",
      "|definitely recommend reasonable generous |0           |55        |-4.02535169073515  |\n",
      "|members content decided try              |0           |54        |-4.007333185232471 |\n",
      "|enjoyed highly recommend anyone          |0           |54        |-4.007333185232471 |\n",
      "|(translated google)  excelente           |1           |109       |-4.007333185232471 |\n",
      "|give consistently towards customers      |0           |54        |-4.007333185232471 |\n",
      "|productive attentive definitely recommend|0           |53        |-3.9889840465642745|\n",
      "|highly recommend recomiendo ampliamente  |0           |53        |-3.9889840465642745|\n",
      "|generous portions reasonable loved       |0           |53        |-3.9889840465642745|\n",
      "|arrived rapidly definitely recommend     |0           |53        |-3.9889840465642745|\n",
      "+-----------------------------------------+------------+----------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4. Create 4-grams\n",
    "# ----------------------------\n",
    "four_grammer = NGram(n=4, inputCol=\"operational_tokens\", outputCol=\"four_grams\")\n",
    "closed_df = four_grammer.transform(closed_df)\n",
    "open_df   = four_grammer.transform(open_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Explode 4-grams and count\n",
    "# ----------------------------\n",
    "def four_gram_count(df, count_col):\n",
    "    return df.select(F.explode(\"four_grams\").alias(\"ngram\")) \\\n",
    "             .groupBy(\"ngram\").count().withColumnRenamed(\"count\", count_col)\n",
    "\n",
    "closed_counts = four_gram_count(closed_df, \"closed_count\")\n",
    "open_counts   = four_gram_count(open_df, \"open_count\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Combine counts and compute log-odds\n",
    "# ----------------------------\n",
    "combined_counts = closed_counts.join(open_counts, on=\"ngram\", how=\"outer\").fillna(0)\n",
    "combined_counts = combined_counts.withColumn(\n",
    "    \"log_odds_ratio\",\n",
    "    F.log((F.col(\"closed_count\")+1) / (F.col(\"open_count\")+1))\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Rank for comparative insight\n",
    "# ----------------------------\n",
    "top_closed_4_grams = combined_counts.orderBy(F.col(\"log_odds_ratio\").desc())\n",
    "top_open_4_grams   = combined_counts.orderBy(F.col(\"log_odds_ratio\").asc())\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Show top results\n",
    "# ----------------------------\n",
    "print(\"4-grams indicative of closed stores:\")\n",
    "top_closed_4_grams.show(50, truncate=False)\n",
    "\n",
    "print(\"4-grams indicative of open stores:\")\n",
    "top_open_4_grams.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009a22e-adf4-404b-b6e9-f73713f9162a",
   "metadata": {},
   "source": [
    "#### Comparative 5-Gram Analysis for Open vs. Closed Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9b93f82-f7c7-465a-856c-08d83c3729b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-grams indicative of closed stores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+------------+----------+------------------+\n",
      "|ngram                                             |closed_count|open_count|log_odds_ratio    |\n",
      "+--------------------------------------------------+------------+----------+------------------+\n",
      "|scammers scammers scammers scammers scammers      |53          |0         |3.9889840465642745|\n",
      "|fluffy fluffy fluffy fluffy fluffy                |15          |0         |2.772588722239781 |\n",
      "|informed refund almost year passed                |4           |0         |1.6094379124341003|\n",
      "|contacted credit informed refund almost           |4           |0         |1.6094379124341003|\n",
      "|refund almost year passed received                |4           |0         |1.6094379124341003|\n",
      "|gambar pertama adalah facebook sebelum            |4           |0         |1.6094379124341003|\n",
      "|called said cheating expensive without            |4           |0         |1.6094379124341003|\n",
      "|ketiga dengan gambar pertama adalah               |4           |0         |1.6094379124341003|\n",
      "|credit informed refund almost year                |4           |0         |1.6094379124341003|\n",
      "|dengan gambar pertama adalah facebook             |4           |0         |1.6094379124341003|\n",
      "|voucher happened approximately careful cashier    |3           |0         |1.3862943611198906|\n",
      "|calls shout customers find personally             |3           |0         |1.3862943611198906|\n",
      "|indeed denied vianni stated fabricated            |3           |0         |1.3862943611198906|\n",
      "|attractive particular recommend arugula dressing  |3           |0         |1.3862943611198906|\n",
      "|week worse verbally assaulted upset               |3           |0         |1.3862943611198906|\n",
      "|tried many honestly heard adding                  |3           |0         |1.3862943611198906|\n",
      "|spend stated uber instead cancelled               |3           |0         |1.3862943611198906|\n",
      "|audio driver recommend anyone spend               |3           |0         |1.3862943611198906|\n",
      "|already mentioned ordeal trying asked             |3           |0         |1.3862943611198906|\n",
      "|traveling eau accidentally seemed entering        |3           |0         |1.3862943611198906|\n",
      "|speaking manager lyn expressed concern            |3           |0         |1.3862943611198906|\n",
      "|recommendations rundown exterior emanating smells |3           |0         |1.3862943611198906|\n",
      "|took credit along debit without                   |3           |0         |1.3862943611198906|\n",
      "|greeted visiting said glad ordered                |3           |0         |1.3862943611198906|\n",
      "|exterior emanating smells including sparkling     |3           |0         |1.3862943611198906|\n",
      "|stopped week surprised find birthday              |3           |0         |1.3862943611198906|\n",
      "|cashier going steal putting wallet                |3           |0         |1.3862943611198906|\n",
      "|constructed ingredient chilled warmer forkful     |3           |0         |1.3862943611198906|\n",
      "|denied vianni stated fabricated cameras           |3           |0         |1.3862943611198906|\n",
      "|concern said follow week decided                  |3           |0         |1.3862943611198906|\n",
      "|bring suggest trying brownie gorgeous             |3           |0         |1.3862943611198906|\n",
      "|stuck previous customers asco disaster            |3           |0         |1.3862943611198906|\n",
      "|generous alcohol ordering issue fairly            |3           |0         |1.3862943611198906|\n",
      "|noisy wait either incredibly charming             |3           |0         |1.3862943611198906|\n",
      "|drinker chosen typically includes otherwise       |3           |0         |1.3862943611198906|\n",
      "|amazingness added rocked definitely minutes       |3           |0         |1.3862943611198906|\n",
      "|decided leave manager decided week                |3           |0         |1.3862943611198906|\n",
      "|stated uber instead cancelled thats               |3           |0         |1.3862943611198906|\n",
      "|smells including sparkling multitudes figurines   |3           |0         |1.3862943611198906|\n",
      "|either incredibly charming toes complete          |3           |0         |1.3862943611198906|\n",
      "|week surprised find birthday seated               |3           |0         |1.3862943611198906|\n",
      "|leave manager decided week work                   |3           |0         |1.3862943611198906|\n",
      "|reviews try give benefit doubt                    |3           |0         |1.3862943611198906|\n",
      "|gave voucher happened approximately careful       |3           |0         |1.3862943611198906|\n",
      "|manager decided week work workers                 |3           |0         |1.3862943611198906|\n",
      "|worse verbally assaulted upset give               |3           |0         |1.3862943611198906|\n",
      "|ordered appies appies porcini sachets             |3           |0         |1.3862943611198906|\n",
      "|including sparkling multitudes figurines customers|3           |0         |1.3862943611198906|\n",
      "|seated ordered appies appies porcini              |3           |0         |1.3862943611198906|\n",
      "|week decided leave manager decided                |3           |0         |1.3862943611198906|\n",
      "+--------------------------------------------------+------------+----------+------------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "5-grams indicative of open stores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6948:==================================================>   (41 + 3) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------+----------+-------------------+\n",
      "|ngram                                              |closed_count|open_count|log_odds_ratio     |\n",
      "+---------------------------------------------------+------------+----------+-------------------+\n",
      "|frickin frickin frickin frickin frickin            |0           |301       |-5.71042701737487  |\n",
      "|lolol lolol lolol lolol lolol                      |0           |291       |-5.676753802268282 |\n",
      "|blah blah blah blah blah                           |0           |78        |-4.3694478524670215|\n",
      "|boner boner boner boner boner                      |0           |70        |-4.2626798770413155|\n",
      "|trying consistently towards visitors often         |1           |133       |-4.204692619390966 |\n",
      "|pacer meme deadthe pacer meme                      |0           |65        |-4.189654742026425 |\n",
      "|meme deadthe pacer meme deadthe                    |0           |65        |-4.189654742026425 |\n",
      "|deadthe pacer meme deadthe pacer                   |0           |64        |-4.174387269895637 |\n",
      "|welcomed felt loved shall definitely               |0           |60        |-4.110873864173311 |\n",
      "|prefere opportunity whenever need comes            |0           |54        |-4.007333185232471 |\n",
      "|rude rude rude rude rude                           |0           |53        |-3.9889840465642745|\n",
      "|deeply impressed handled correctly hassles         |0           |48        |-3.891820298110627 |\n",
      "|consistently towards customers frequently highly   |0           |47        |-3.871201010907891 |\n",
      "|work customer organized rates affordable           |0           |47        |-3.871201010907891 |\n",
      "|opportunity everytime need comes instantly         |0           |47        |-3.871201010907891 |\n",
      "|consistently towards clients often recommend       |5           |284       |-3.8607297110405954|\n",
      "|give consistently towards clients often            |0           |46        |-3.8501476017100584|\n",
      "|work customer kept rates reasonable                |0           |45        |-3.828641396489095 |\n",
      "|prepare consistently towards guests often          |0           |44        |-3.8066624897703196|\n",
      "|provide consistently towards clients often         |0           |44        |-3.8066624897703196|\n",
      "|horrible horrible horrible horrible horrible       |1           |87        |-3.784189633918261 |\n",
      "|give consistently towards customers often          |0           |42        |-3.7612001156935624|\n",
      "|trying towards customers often highly              |0           |42        |-3.7612001156935624|\n",
      "|provide consistently towards customers often       |0           |41        |-3.7376696182833684|\n",
      "|said said said said said                           |0           |41        |-3.7376696182833684|\n",
      "|towards customers frequently highly recommend      |0           |40        |-3.713572066704308 |\n",
      "|surely recommend convenient rates generous         |0           |40        |-3.713572066704308 |\n",
      "|enjoyed shall definitely highly recommend          |0           |39        |-3.6888794541139363|\n",
      "|work customer organized rates convenient           |0           |39        |-3.6888794541139363|\n",
      "|provide towards customers often recommend          |0           |36        |-3.6109179126442243|\n",
      "|trying consistently towards guests frequently      |0           |36        |-3.6109179126442243|\n",
      "|professional put shall surely highly               |0           |36        |-3.6109179126442243|\n",
      "|easily recommend convenient rates generous         |0           |36        |-3.6109179126442243|\n",
      "|nasty nasty nasty nasty nasty                      |0           |36        |-3.6109179126442243|\n",
      "|preferred places customer highly recommend         |0           |36        |-3.6109179126442243|\n",
      "|portions affordable consistently highly recommend  |0           |36        |-3.6109179126442243|\n",
      "|warmly welcomed members felt loved                 |0           |35        |-3.58351893845611  |\n",
      "|trying consistently towards customers frequently   |0           |34        |-3.5553480614894135|\n",
      "|consistently towards guests often highly           |3           |138       |-3.548179572010801 |\n",
      "|rates productive amiable highly recommend          |0           |33        |-3.5263605246161616|\n",
      "|opportunity everytime need comes straightaway      |0           |33        |-3.5263605246161616|\n",
      "|portions affordable regularly highly recommend     |0           |33        |-3.5263605246161616|\n",
      "|consistently towards guests frequently recommend   |1           |66        |-3.5115454388310208|\n",
      "|consistently towards visitors often highly         |3           |133       |-3.5115454388310208|\n",
      "|prepare trying consistently towards customers      |0           |32        |-3.4965075614664802|\n",
      "|consistently towards customers frequently recommend|1           |65        |-3.4965075614664802|\n",
      "|adore consistently towards visitors often          |0           |32        |-3.4965075614664802|\n",
      "|favourite rapidly rates definitely recommend       |0           |32        |-3.4965075614664802|\n",
      "|recommend trying delightful customer highly        |0           |32        |-3.4965075614664802|\n",
      "|opportunity everytime need decent comes            |0           |32        |-3.4965075614664802|\n",
      "+---------------------------------------------------+------------+----------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4. Create 5-grams\n",
    "# ----------------------------\n",
    "five_grammer = NGram(n=5, inputCol=\"operational_tokens\", outputCol=\"five_grams\")\n",
    "closed_df = five_grammer.transform(closed_df)\n",
    "open_df   = five_grammer.transform(open_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Explode 5-grams and count\n",
    "# ----------------------------\n",
    "def five_gram_count(df, count_col):\n",
    "    return df.select(F.explode(\"five_grams\").alias(\"ngram\")) \\\n",
    "             .groupBy(\"ngram\").count().withColumnRenamed(\"count\", count_col)\n",
    "\n",
    "closed_counts = five_gram_count(closed_df, \"closed_count\")\n",
    "open_counts   = five_gram_count(open_df, \"open_count\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Combine counts and compute log-odds\n",
    "# ----------------------------\n",
    "combined_counts = closed_counts.join(open_counts, on=\"ngram\", how=\"outer\").fillna(0)\n",
    "combined_counts = combined_counts.withColumn(\n",
    "    \"log_odds_ratio\",\n",
    "    F.log((F.col(\"closed_count\")+1) / (F.col(\"open_count\")+1))\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Rank for comparative insight\n",
    "# ----------------------------\n",
    "top_closed_5_grams = combined_counts.orderBy(F.col(\"log_odds_ratio\").desc())\n",
    "top_open_5_grams   = combined_counts.orderBy(F.col(\"log_odds_ratio\").asc())\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Show top results\n",
    "# ----------------------------\n",
    "print(\"5-grams indicative of closed stores:\")\n",
    "top_closed_5_grams.show(50, truncate=False)\n",
    "\n",
    "print(\"5-grams indicative of open stores:\")\n",
    "top_open_5_grams.show(50, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
